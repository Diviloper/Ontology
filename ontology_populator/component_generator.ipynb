{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-16T06:43:09.411183700Z",
     "start_time": "2023-05-16T06:43:09.324575900Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "from pyshacl import validate\n",
    "from rdflib import *\n",
    "from rdflib.collection import Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ontology = Graph()\n",
    "ontology.parse(r'../ontologies/ontology_full.ttl')\n",
    "\n",
    "dmop = Namespace('http://www.e-lico.eu/ontologies/dmo/DMOP/DMOP.owl#')\n",
    "bigowl_data = Namespace('https://w3id.org/BIGOWLData/')\n",
    "bigowl_algorithms = Namespace('https://w3id.org/BIGOWLAlgorithms/')\n",
    "bigowl_problems = Namespace('https://w3id.org/BIGOWLProblems/')\n",
    "bigowl_workflows = Namespace('https://w3id.org/BIGOWLWorkflows/')\n",
    "do = Namespace('https://diviloper.dev/ontology#')\n",
    "dc = Namespace('https://diviloper.dev/ontology/components#')\n",
    "dr = Namespace('https://diviloper.dev/ontology/restrictions#')\n",
    "dru = Namespace('https://diviloper.dev/ontology/rules#')\n",
    "da = Namespace('https://diviloper.dev/ABox#')\n",
    "\n",
    "\n",
    "def get_graph():\n",
    "    g = Graph()\n",
    "    g.bind('dmop', dmop)\n",
    "    g.bind('bod', bigowl_data)\n",
    "    g.bind('boa', bigowl_algorithms)\n",
    "    g.bind('bop', bigowl_problems)\n",
    "    g.bind('bow', bigowl_workflows)\n",
    "    g.bind('do', do)\n",
    "    g.bind('dc', dc)\n",
    "    g.bind('dr', dr)\n",
    "    g.bind('da', da)\n",
    "    return g"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T06:43:09.445139700Z",
     "start_time": "2023-05-16T06:43:09.415113200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_subgraph(graph, node):\n",
    "    subgraph = Graph()\n",
    "    visited_nodes = set()\n",
    "    to_visit = [node]\n",
    "    mappings = {}\n",
    "\n",
    "    while to_visit:\n",
    "        current_node = to_visit.pop()\n",
    "        visited_nodes.add(current_node)\n",
    "        for s, p, o in graph.triples((current_node, None, None)):\n",
    "            if (s, RDF.type, SH.NodeShape) in graph and s != node:\n",
    "                if s not in mappings:\n",
    "                    mappings[s] = BNode()\n",
    "                s = mappings[s]\n",
    "\n",
    "            subgraph.add((s, p, o))\n",
    "            if o not in visited_nodes:\n",
    "                to_visit.append(o)\n",
    "\n",
    "    return subgraph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-03T11:27:15.573300200Z",
     "start_time": "2023-05-03T11:27:15.562568400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def add_component(component_graph, component, ctype, algorithm, inputs, outputs, parameters):\n",
    "    component_graph.add((component, RDF.type, ctype))\n",
    "    component_graph.add((component, bigowl_workflows.hasAlgorithm, algorithm))\n",
    "    component_graph.add((component, bigowl_workflows.numberOfInputs, Literal(len(inputs))))\n",
    "    component_graph.add((component, bigowl_workflows.numberOfOutputs, Literal(len(outputs))))\n",
    "    for i in inputs:\n",
    "        component_graph.add((component, bigowl_workflows.specifiesInputClass, i))\n",
    "    for o in outputs:\n",
    "        component_graph.add((component, bigowl_workflows.specifiesOutputClass, o))\n",
    "\n",
    "    for param, pname, ptype, default in parameters:\n",
    "        component_graph.add((param, RDF.type, dmop.Parameter))\n",
    "        component_graph.add((component, bigowl_workflows.hasParameter, param))\n",
    "        component_graph.add((param, bigowl_workflows.hasName, Literal(pname)))\n",
    "        component_graph.add((param, bigowl_workflows.hasDataType, ptype))\n",
    "        component_graph.add((param, bigowl_workflows.hasDefaultValue, Literal(default)))\n",
    "        component_graph.add((component, bigowl_workflows.hasParameter, param))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T06:43:09.706797900Z",
     "start_time": "2023-05-16T06:43:09.687305Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def add_bnode(graph, triples: List[Tuple[URIRef, Union[URIRef, Literal]]]):\n",
    "    bnode = BNode()\n",
    "    for p, o in triples:\n",
    "        graph.add((bnode, p, o))\n",
    "    return bnode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T06:43:10.953892400Z",
     "start_time": "2023-05-16T06:43:10.942671500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=Ncc80f013c495430abf1c39eb86d29fe4 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restrictions = get_graph()\n",
    "\n",
    "restrictions.add((dr.DataTag, RDF.type, OWL.Class))\n",
    "\n",
    "restrictions.serialize('restrictions.ttl', format='ttl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T07:20:44.557597Z",
     "start_time": "2023-05-16T07:20:44.541802400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=Ncc80f013c495430abf1c39eb86d29fe4 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NonNullNumericFeatureColumnShape\n",
    "column_shape = dr.NonNullNumericFeatureColumnShape\n",
    "# column_shape = BNode()\n",
    "restrictions.add((column_shape, RDF.type, SH.NodeShape))\n",
    "\n",
    "numeric_column_property = dr.NumericColumnProperty\n",
    "# numeric_column_property = BNode()\n",
    "restrictions.add((numeric_column_property, SH.path, bigowl_data.hasDataPrimitiveTypeColumn))\n",
    "restrictions.add((numeric_column_property, SH['in'],\n",
    "                  Collection(restrictions, BNode(), seq=[bigowl_data.Integer, bigowl_data.Float]).uri))\n",
    "\n",
    "non_null_column_property = dr.NonNullColumnProperty\n",
    "# non_null_column_property = BNode()\n",
    "restrictions.add((non_null_column_property, SH.path, do.containsNulls))\n",
    "restrictions.add((non_null_column_property, SH.datatype, XSD.boolean))\n",
    "restrictions.add((non_null_column_property, SH.hasValue, Literal(False)))\n",
    "\n",
    "feature_column_property = dr.FeatureColumnProperty\n",
    "# feature_column_property = BNode()\n",
    "restrictions.add((feature_column_property, SH.path, do.isFeature))\n",
    "restrictions.add((feature_column_property, SH.datatype, XSD.boolean))\n",
    "restrictions.add((feature_column_property, SH.hasValue, Literal(True)))\n",
    "\n",
    "feature_column = dr.FeatureColumnShape\n",
    "# feature_column = BNode()\n",
    "restrictions.add((feature_column, RDF.type, SH.NodeShape))\n",
    "restrictions.add((feature_column, SH.targetClass, bigowl_data.Column))\n",
    "restrictions.add((feature_column, SH.property, feature_column_property))\n",
    "\n",
    "restrictions.add((column_shape, SH.property, numeric_column_property))\n",
    "restrictions.add((column_shape, SH.property, non_null_column_property))\n",
    "restrictions.add((column_shape, SH.targetClass, feature_column))\n",
    "\n",
    "# NonNullNumericFeatureTabularDatasetShape\n",
    "non_null_numeric_tabular_dataset_shape = dr.NonNullNumericFeatureTabularDatasetShape\n",
    "restrictions.add((non_null_numeric_tabular_dataset_shape, RDF.type, SH.NodeShape))\n",
    "restrictions.add((non_null_numeric_tabular_dataset_shape, SH.targetClass, do.TabularDataset))\n",
    "\n",
    "bnode = BNode()\n",
    "restrictions.add((bnode, SH.path, bigowl_data.hasColumn))\n",
    "restrictions.add((bnode, SH.node, column_shape))\n",
    "\n",
    "restrictions.add((non_null_numeric_tabular_dataset_shape, SH.property, bnode))\n",
    "\n",
    "# LabeledTabularDatasetShape\n",
    "\n",
    "label_column_property = dr.LabelColumnProperty\n",
    "restrictions.add((label_column_property, SH.path, do.isLabel))\n",
    "restrictions.add((label_column_property, SH.datatype, XSD.boolean))\n",
    "restrictions.add((label_column_property, SH.hasValue, Literal(True)))\n",
    "\n",
    "label_column_shape = dr.LabelColumnShape\n",
    "restrictions.add((label_column_shape, RDF.type, SH.NodeShape))\n",
    "restrictions.add((label_column_shape, SH.targetClass, bigowl_data.Column))\n",
    "restrictions.add((label_column_shape, SH.property, label_column_property))\n",
    "\n",
    "labeled_dataset_shape = dr.LabeledTabularDatasetShape\n",
    "restrictions.add((labeled_dataset_shape, RDF.type, SH.NodeShape))\n",
    "restrictions.add((labeled_dataset_shape, SH.targetClass, do.TabularDataset))\n",
    "\n",
    "bnode_qualified = BNode()\n",
    "restrictions.add((bnode_qualified, SH.path, do.isLabel))\n",
    "restrictions.add((bnode_qualified, SH.hasValue, Literal(True)))\n",
    "\n",
    "bnode_column = BNode()\n",
    "restrictions.add((bnode_column, SH.path, bigowl_data.hasColumn))\n",
    "restrictions.add((bnode_column, SH.qualifiedValueShape, bnode_qualified))\n",
    "restrictions.add((bnode_column, SH.qualifiedMinCount, Literal(1)))\n",
    "restrictions.add((bnode_column, SH.minCount, Literal(1)))\n",
    "\n",
    "restrictions.add((labeled_dataset_shape, SH.property, bnode_column))\n",
    "\n",
    "restrictions.serialize('restrictions.ttl', format='ttl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T07:20:45.163894400Z",
     "start_time": "2023-05-16T07:20:45.138957600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=Ncc80f013c495430abf1c39eb86d29fe4 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_null_column_shape = dr.NonNullColumnShape\n",
    "restrictions.add((non_null_column_shape, RDF.type, SH.NodeShape))\n",
    "restrictions.add((non_null_column_shape, SH.targetClass, bigowl_data.Column))\n",
    "restrictions.add((non_null_column_shape, SH.property, non_null_column_property))\n",
    "\n",
    "bnode = BNode()\n",
    "restrictions.add((bnode, SH.path, bigowl_data.hasColumn))\n",
    "restrictions.add((bnode, SH.node, non_null_column_shape))\n",
    "\n",
    "non_null_tabular_dataset_shape = dr.NonNullTabularDatasetShape\n",
    "restrictions.add((non_null_tabular_dataset_shape, RDF.type, SH.NodeShape))\n",
    "restrictions.add((non_null_tabular_dataset_shape, SH.targetClass, do.TabularDataset))\n",
    "restrictions.add((non_null_tabular_dataset_shape, SH.property, bnode))\n",
    "\n",
    "tabular_dataset_shape = dr.TabularDatasetShape\n",
    "restrictions.add((tabular_dataset_shape, RDF.type, SH.NodeShape))\n",
    "restrictions.add((tabular_dataset_shape, SH.targetClass, do.TabularDataset))\n",
    "\n",
    "numeric_column_shape = dr.NumericColumnShape\n",
    "restrictions.add((numeric_column_shape, RDF.type, SH.NodeShape))\n",
    "restrictions.add((numeric_column_shape, SH.targetClass, bigowl_data.Column))\n",
    "restrictions.add((numeric_column_shape, SH.property, numeric_column_property))\n",
    "\n",
    "bnode = BNode()\n",
    "restrictions.add((bnode, SH.path, bigowl_data.hasColumn))\n",
    "restrictions.add((bnode, SH.node, numeric_column_shape))\n",
    "\n",
    "numeric_tabular_dataset_shape = dr.NumericTabularDatasetShape\n",
    "restrictions.add((numeric_tabular_dataset_shape, RDF.type, SH.NodeShape))\n",
    "restrictions.add((numeric_tabular_dataset_shape, SH.targetClass, do.TabularDataset))\n",
    "restrictions.add((numeric_tabular_dataset_shape, SH.property, bnode))\n",
    "\n",
    "restrictions.serialize('restrictions.ttl', format='ttl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T07:20:45.684262100Z",
     "start_time": "2023-05-16T07:20:45.656129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=Ncc80f013c495430abf1c39eb86d29fe4 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_tabular_dataset_shape = dr.NormalizedTabularDatasetShape\n",
    "restrictions.add((normalized_tabular_dataset_shape, RDF.type, SH.NodeShape))\n",
    "restrictions.add((normalized_tabular_dataset_shape, RDF.type, dr.DataTag))\n",
    "restrictions.add((normalized_tabular_dataset_shape, SH.targetClass, do.TabularDataset))\n",
    "\n",
    "restrictions.serialize('restrictions.ttl', format='ttl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T07:21:38.819502400Z",
     "start_time": "2023-05-16T07:21:38.795496100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=Ncc80f013c495430abf1c39eb86d29fe4 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_input_shape = dr.KMeansInputShape\n",
    "restrictions.add((kmeans_input_shape, RDF.type, SH.NodeShape))\n",
    "restrictions.add((kmeans_input_shape, SH.targetClass, do.TabularDataset))\n",
    "\n",
    "restrictions.add((kmeans_input_shape, SH['and'], Collection(restrictions, BNode(), seq=[non_null_tabular_dataset_shape, numeric_tabular_dataset_shape]).uri))\n",
    "\n",
    "restrictions.serialize('restrictions.ttl', format='ttl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T07:26:51.897314100Z",
     "start_time": "2023-05-16T07:26:51.868028100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=Ncc80f013c495430abf1c39eb86d29fe4 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_outlier_factor_input_shape = dr.LocalOutlierFactorInputShape\n",
    "restrictions.add((local_outlier_factor_input_shape, RDF.type, SH.NodeShape))\n",
    "restrictions.add((local_outlier_factor_input_shape, SH.targetClass, do.TabularDataset))\n",
    "\n",
    "restrictions.add((local_outlier_factor_input_shape, SH['and'], Collection(restrictions, BNode(), seq=[non_null_tabular_dataset_shape, numeric_tabular_dataset_shape, normalized_tabular_dataset_shape]).uri))\n",
    "\n",
    "restrictions.serialize('restrictions.ttl', format='ttl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T07:26:31.789898900Z",
     "start_time": "2023-05-16T07:26:31.775699200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreValidator\n",
      "Validation Report\n",
      "Conforms: False\n",
      "Results (3):\n",
      "Constraint Violation in NodeConstraintComponent (http://www.w3.org/ns/shacl#NodeConstraintComponent):\n",
      "\tSeverity: sh:Violation\n",
      "\tSource Shape: [ sh:node dr:NumericColumnShape ; sh:path bod:hasColumn ]\n",
      "\tFocus Node: dab:penguins.csv\n",
      "\tValue Node: <https://diviloper.dev/ABox#penguins.csv/sex>\n",
      "\tResult Path: bod:hasColumn\n",
      "\tMessage: Value does not conform to Shape dr:NumericColumnShape\n",
      "Constraint Violation in NodeConstraintComponent (http://www.w3.org/ns/shacl#NodeConstraintComponent):\n",
      "\tSeverity: sh:Violation\n",
      "\tSource Shape: [ sh:node dr:NumericColumnShape ; sh:path bod:hasColumn ]\n",
      "\tFocus Node: dab:penguins.csv\n",
      "\tValue Node: <https://diviloper.dev/ABox#penguins.csv/island>\n",
      "\tResult Path: bod:hasColumn\n",
      "\tMessage: Value does not conform to Shape dr:NumericColumnShape\n",
      "Constraint Violation in NodeConstraintComponent (http://www.w3.org/ns/shacl#NodeConstraintComponent):\n",
      "\tSeverity: sh:Violation\n",
      "\tSource Shape: [ sh:node dr:NumericColumnShape ; sh:path bod:hasColumn ]\n",
      "\tFocus Node: dab:penguins.csv\n",
      "\tValue Node: <https://diviloper.dev/ABox#penguins.csv/species>\n",
      "\tResult Path: bod:hasColumn\n",
      "\tMessage: Value does not conform to Shape dr:NumericColumnShape\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load your RDF data file\n",
    "data_graph = Graph()\n",
    "data_graph.parse('../dataset_annotator/penguins_annotated.ttl', format='ttl')\n",
    "\n",
    "conforms, result_graph, results = validate(data_graph, shacl_graph=restrictions,\n",
    "                                           validate_shapes=[numeric_tabular_dataset_shape],\n",
    "                                           focus=da.term('penguins.csv'))\n",
    "\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T11:34:24.694305Z",
     "end_time": "2023-04-25T11:34:24.708313Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=Nebb9a62205934be382d4b470874d80b9 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components = get_graph()\n",
    "\n",
    "add_component(\n",
    "    components,\n",
    "    do.KMeansComponent,\n",
    "    bigowl_workflows.DataAnalysing,\n",
    "    do.KMeans,\n",
    "    [kmeans_input_shape],\n",
    "    [do.ClusteredDataset, do.Centroids],\n",
    "    [(do.NumberOfClusters, Literal('Number of Clusters'), bigowl_data.Integer, Literal(3))])\n",
    "\n",
    "add_component(\n",
    "    components,\n",
    "    do.DropNullsComponent,\n",
    "    bigowl_workflows.DataProcessing,\n",
    "    do.DropNulls,\n",
    "    [tabular_dataset_shape],\n",
    "    [non_null_tabular_dataset_shape],\n",
    "    []\n",
    ")\n",
    "\n",
    "add_component(\n",
    "    components,\n",
    "    do.StandardizeComponent,\n",
    "    bigowl_workflows.DataProcessing,\n",
    "    do.Standardize,\n",
    "    [numeric_tabular_dataset_shape],\n",
    "    [normalized_tabular_dataset_shape],\n",
    "    []\n",
    ")\n",
    "\n",
    "add_component(\n",
    "    components,\n",
    "    do.MinMaxScalerComponent,\n",
    "    bigowl_workflows.DataProcessing,\n",
    "    do.MinMaxScaler,\n",
    "    [numeric_tabular_dataset_shape],\n",
    "    [normalized_tabular_dataset_shape],\n",
    "    []\n",
    ")\n",
    "\n",
    "add_component(\n",
    "    components,\n",
    "    do.RobustScalerComponent,\n",
    "    bigowl_workflows.DataProcessing,\n",
    "    do.RobustScaler,\n",
    "    [numeric_tabular_dataset_shape],\n",
    "    [normalized_tabular_dataset_shape],\n",
    "    []\n",
    ")\n",
    "\n",
    "add_component(\n",
    "    components,\n",
    "    do.ConvertToNumericComponent,\n",
    "    bigowl_workflows.DataProcessing,\n",
    "    do.ConvertToNumeric,\n",
    "    [tabular_dataset_shape],\n",
    "    [numeric_tabular_dataset_shape],\n",
    "    []\n",
    ")\n",
    "\n",
    "add_component(\n",
    "    components,\n",
    "    do.LocalOutlierFactorComponent,\n",
    "    bigowl_workflows.DataAnalysing,\n",
    "    do.LocalOutlierFactor,\n",
    "    [local_outlier_factor_input_shape],\n",
    "    [do.LabeledDataset],\n",
    "    [(do.NumberOfNeighbors, Literal('Number of Neighbors'), bigowl_data.Integer, Literal(20)),\n",
    "     (do.Contamination, Literal('Contamination'), bigowl_data.Float, Literal(0.1))]\n",
    ")\n",
    "components.serialize('components.ttl', format='ttl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T07:33:13.632960400Z",
     "start_time": "2023-05-16T07:33:13.600953600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_graph = get_graph()\n",
    "output_graph = get_graph()\n",
    "\n",
    "input_dataset = da.term('penguins.csv')\n",
    "output_dataset = da.term('penguins_non_null.csv')\n",
    "\n",
    "for s, p, o in input_graph.triples((input_dataset, bigowl_data.hasColumn, None)):\n",
    "    output_graph.add((output_dataset, bigowl_data.hasColumn, o))\n",
    "    for s2, p2, o2 in input_graph.triples((o, None, None)):\n",
    "        if p2 == do.containsNulls:\n",
    "            output_graph.add((o, p2, Literal(False)))\n",
    "        else:\n",
    "            output_graph.add((o, p2, o2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Workflow examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=N1ad235b07f45405a9a8882690f4ca151 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow_graph = get_graph()\n",
    "workflow = da.term('workflow_test')\n",
    "workflow_graph.add((workflow, RDF.type, bigowl_workflows.Workflow))\n",
    "\n",
    "load_task = da.term('load_task')\n",
    "workflow_graph.add((load_task, RDF.type, bigowl_workflows.Task))\n",
    "workflow_graph.add((load_task, bigowl_workflows.hasComponent, do.Import_CSV))\n",
    "workflow_graph.add((load_task, bigowl_workflows.order, Literal(1)))\n",
    "workflow_graph.add((load_task, bigowl_workflows.hasOutput, da.term('penguins.csv')))\n",
    "workflow_graph.add((load_task, do.hasParameterValue,\n",
    "                    add_bnode(workflow_graph, [\n",
    "                        (do.forParameter, do.URL_Param),\n",
    "                        (do.hasValue, Literal(r'/workflows/test/penguins.csv'))])))\n",
    "\n",
    "drop_nulls_task = da.term('drop_nulls_task')\n",
    "mid_dataset = add_bnode(workflow_graph, [(RDF.type, do.TabularDataset), (do.conformsTo, non_null_tabular_dataset_shape)])\n",
    "workflow_graph.add((drop_nulls_task, RDF.type, bigowl_workflows.Task))\n",
    "workflow_graph.add((drop_nulls_task, bigowl_workflows.hasComponent, do.DropNullsComponent))\n",
    "workflow_graph.add((drop_nulls_task, bigowl_workflows.order, Literal(2)))\n",
    "workflow_graph.add((drop_nulls_task, bigowl_workflows.hasInput, da.term('penguins.csv')))\n",
    "workflow_graph.add((drop_nulls_task, bigowl_workflows.hasOutput, mid_dataset))\n",
    "workflow_graph.add((drop_nulls_task, do.hasParameterValue,\n",
    "                    add_bnode(workflow_graph, [\n",
    "                        (do.forParameter, do.dropNullsMethod),\n",
    "                        (do.hasValue, Literal('drop_rows'))])))\n",
    "\n",
    "convert_to_numeric_task = da.term('convert_to_numeric_task')\n",
    "mid_dataset2 = add_bnode(workflow_graph, [(RDF.type, do.TabularDataset), (do.conformsTo, numeric_tabular_dataset_shape), (do.conformsTo, non_null_tabular_dataset_shape)])\n",
    "workflow_graph.add((convert_to_numeric_task, RDF.type, bigowl_workflows.Task))\n",
    "workflow_graph.add((convert_to_numeric_task, bigowl_workflows.hasComponent, do.ConvertToNumericComponent))\n",
    "workflow_graph.add((convert_to_numeric_task, bigowl_workflows.order, Literal(3)))\n",
    "workflow_graph.add((convert_to_numeric_task, bigowl_workflows.hasInput, mid_dataset))\n",
    "workflow_graph.add((convert_to_numeric_task, bigowl_workflows.hasOutput, mid_dataset2))\n",
    "\n",
    "kmeans_task = da.term('kmeans_task')\n",
    "final_dataset = add_bnode(workflow_graph, [(RDF.type, do.TabularDataset), (do.conformsTo, labeled_dataset_shape)])\n",
    "clusters = add_bnode(workflow_graph, [(RDF.type, do.TabularDataset), (do.conformsTo, labeled_dataset_shape)])\n",
    "\n",
    "workflow_graph.add((kmeans_task, RDF.type, bigowl_workflows.Task))\n",
    "workflow_graph.add((kmeans_task, bigowl_workflows.hasComponent, do.KMeansComponent))\n",
    "workflow_graph.add((kmeans_task, bigowl_workflows.order, Literal(4)))\n",
    "workflow_graph.add((kmeans_task, bigowl_workflows.hasInput, mid_dataset2))\n",
    "workflow_graph.add((kmeans_task, bigowl_workflows.hasOutput, final_dataset))\n",
    "workflow_graph.add((kmeans_task, bigowl_workflows.hasOutput, clusters))\n",
    "workflow_graph.add((kmeans_task, do.hasParameterValue,\n",
    "                    add_bnode(workflow_graph, [\n",
    "                        (do.forParameter, do.NumberOfClusters),\n",
    "                        (do.hasValue, Literal(3))])))\n",
    "\n",
    "store_as_csv_task = da.term('store_as_csv_task')\n",
    "workflow_graph.add((store_as_csv_task, RDF.type, bigowl_workflows.Task))\n",
    "workflow_graph.add((store_as_csv_task, bigowl_workflows.hasComponent, do.Export_CSV))\n",
    "workflow_graph.add((store_as_csv_task, bigowl_workflows.order, Literal(5)))\n",
    "workflow_graph.add((store_as_csv_task, bigowl_workflows.hasInput, final_dataset))\n",
    "workflow_graph.add((store_as_csv_task, do.hasParameterValue,\n",
    "                    add_bnode(workflow_graph, [\n",
    "                        (do.forParameter, do.URL_Param),\n",
    "                        (do.hasValue, Literal(r'/workflows/test/clustered_penguins.csv'))])))\n",
    "\n",
    "\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, load_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, drop_nulls_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, convert_to_numeric_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, kmeans_task))\n",
    "\n",
    "workflow_graph.add((load_task, do.connectedTo, drop_nulls_task))\n",
    "workflow_graph.add((drop_nulls_task, do.connectedTo, convert_to_numeric_task))\n",
    "workflow_graph.add((convert_to_numeric_task, do.connectedTo, kmeans_task))\n",
    "\n",
    "workflow_graph.serialize('workflow_test.ttl', format='ttl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T16:54:21.784712Z",
     "end_time": "2023-04-25T16:54:21.801711Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=Nfa894e17782548a3b36ad57c0ced26de (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow_graph = get_graph()\n",
    "\n",
    "\n",
    "workflow = da.term('workflow_test')\n",
    "workflow_graph.add((workflow, RDF.type, bigowl_workflows.Workflow))\n",
    "\n",
    "\n",
    "load_task = da.term('load_task')\n",
    "workflow_graph.add((load_task, bigowl_workflows.hasComponent, do.Import_CSV))\n",
    "\n",
    "drop_nulls_task = da.term('drop_nulls_task')\n",
    "workflow_graph.add((drop_nulls_task, bigowl_workflows.hasComponent, do.DropNullRowsComponent))\n",
    "\n",
    "convert_to_numeric_task = da.term('convert_to_numeric_task')\n",
    "workflow_graph.add((convert_to_numeric_task, bigowl_workflows.hasComponent, do.OneHotEncoderComponent))\n",
    "\n",
    "robust_scaler_task = da.term('robust_scaler_task')\n",
    "workflow_graph.add((robust_scaler_task, bigowl_workflows.hasComponent, do.RobustScalerComponent))\n",
    "\n",
    "local_outlier_factor_task = da.term('local_outlier_factor_task')\n",
    "workflow_graph.add((local_outlier_factor_task, bigowl_workflows.hasComponent, do.LocalOutlierFactorComponent))\n",
    "\n",
    "store_as_csv_task = da.term('store_as_csv_task')\n",
    "workflow_graph.add((store_as_csv_task, bigowl_workflows.hasComponent, do.Export_CSV))\n",
    "\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, load_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, drop_nulls_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, convert_to_numeric_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, robust_scaler_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, local_outlier_factor_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, store_as_csv_task))\n",
    "\n",
    "workflow_graph.add((load_task, do.connectedTo, drop_nulls_task))\n",
    "workflow_graph.add((drop_nulls_task, do.connectedTo, convert_to_numeric_task))\n",
    "workflow_graph.add((convert_to_numeric_task, do.connectedTo, robust_scaler_task))\n",
    "workflow_graph.add((robust_scaler_task, do.connectedTo, local_outlier_factor_task))\n",
    "workflow_graph.add((local_outlier_factor_task, do.connectedTo, store_as_csv_task))\n",
    "\n",
    "workflow_graph.serialize('workflow_test1.ttl', format='ttl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T07:59:58.789149700Z",
     "start_time": "2023-05-16T07:59:58.761512900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=N2142e13a19a74f7c93156ff057f2eb7b (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow_graph = get_graph()\n",
    "\n",
    "\n",
    "workflow = da.term('workflow_test')\n",
    "workflow_graph.add((workflow, RDF.type, bigowl_workflows.Workflow))\n",
    "\n",
    "\n",
    "load_task = da.term('load_task')\n",
    "workflow_graph.add((load_task, bigowl_workflows.hasComponent, do.Import_CSV))\n",
    "\n",
    "drop_nulls_task = da.term('drop_nulls_task')\n",
    "workflow_graph.add((drop_nulls_task, bigowl_workflows.hasComponent, do.MeanImputationComponent))\n",
    "\n",
    "convert_to_numeric_task = da.term('convert_to_numeric_task')\n",
    "workflow_graph.add((convert_to_numeric_task, bigowl_workflows.hasComponent, do.LabelEncoderComponent))\n",
    "\n",
    "robust_scaler_task = da.term('robust_scaler_task')\n",
    "workflow_graph.add((robust_scaler_task, bigowl_workflows.hasComponent, do.RobustScalerComponent))\n",
    "\n",
    "local_outlier_factor_task = da.term('local_outlier_factor_task')\n",
    "workflow_graph.add((local_outlier_factor_task, bigowl_workflows.hasComponent, do.LocalOutlierFactorComponent))\n",
    "\n",
    "store_as_csv_task = da.term('store_as_csv_task')\n",
    "workflow_graph.add((store_as_csv_task, bigowl_workflows.hasComponent, do.Export_CSV))\n",
    "\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, load_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, drop_nulls_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, convert_to_numeric_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, robust_scaler_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, local_outlier_factor_task))\n",
    "workflow_graph.add((workflow, bigowl_workflows.hasTask, store_as_csv_task))\n",
    "\n",
    "workflow_graph.add((load_task, do.connectedTo, drop_nulls_task))\n",
    "workflow_graph.add((drop_nulls_task, do.connectedTo, convert_to_numeric_task))\n",
    "workflow_graph.add((convert_to_numeric_task, do.connectedTo, robust_scaler_task))\n",
    "workflow_graph.add((robust_scaler_task, do.connectedTo, local_outlier_factor_task))\n",
    "workflow_graph.add((local_outlier_factor_task, do.connectedTo, store_as_csv_task))\n",
    "\n",
    "workflow_graph.serialize('workflow_test2.ttl', format='ttl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T07:59:48.020977300Z",
     "start_time": "2023-05-16T07:59:47.974415400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test with SHACL Rules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rules = get_graph()\n",
    "\n",
    "drop_nulls_rule = dru.term('drop_nulls_rule')\n",
    "rules.add((drop_nulls_rule, RDF.type, SH.Rule))\n",
    "rules.add((drop_nulls_rule, SH.targetClass, bigowl_workflows.Task))\n",
    "rules.add((drop_nulls_rule, SH.condition, add_bnode(rules, [\n",
    "    (SH.property, bigowl_workflows.hasComponent),\n",
    "    (SH.value, do.DropNullsComponent),\n",
    "    (SH.minCount, Literal(1)),\n",
    "    (SH.maxCount, Literal(1)),\n",
    "    (SH.message, Literal('DropNullsComponent must have exactly one input'))\n",
    "])))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
